{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medical_specialty_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca-martial/medical-specialty/blob/main/medical_specialty_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9cgIX-NmI4G"
      },
      "source": [
        "## Medical Specialty Prediction with Spark NLP\n",
        "\n",
        "The goal of this project was to predict medical specialties (surgery, internal medicine, medical records, other) based on a corpus of 4999 medical transcriptions using Spark NLP. The corpus was scraped by [Tara Boyle](https://github.com/terrah27) from a [Transcribed Medical Transcription Sample Reports and Examples website](https://mtsamples.com/) and published on [Kaggle](https://www.kaggle.com/tboyle10/medicaltranscriptions). The version used in this project was compiled by [Carlos Salgado](https://github.com/socd06) for Natural Language Processing using the scraped corpus and custom-generated clinical stop words and vocabulary. This compiled version was published on [GitHub](https://github.com/socd06/medical-nlp) and is free to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81xX4FYSovpb"
      },
      "source": [
        "## Set-Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlQPMy6kl_Ez"
      },
      "source": [
        "### Installing SparkNLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "P2Vg-VepNRNw",
        "outputId": "d531c5c8-1fdb-471f-f64d-e6da4147d7ac"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "\n",
        "# Adding license key-value pairs to environment variables\n",
        "os.environ.update(license_keys)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c70a1f8-ded9-4c1b-b3fc-b179d83a4d5b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9c70a1f8-ded9-4c1b-b3fc-b179d83a4d5b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spark_nlp_for_healthcare_spark_ocr_3138.json to spark_nlp_for_healthcare_spark_ocr_3138.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWos3ASBBKF8",
        "outputId": "36f9ea0b-4aae-4588-e215-1d3b70998704"
      },
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 116 kB 66.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 70.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 136 kB 10.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 122 kB 8.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "U40sDKOHNYe9",
        "outputId": "f7e14e01-9294-437d-e359-f2522f332603"
      },
      "source": [
        "# Import libraries and start session\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\", \n",
        "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
        "          \"spark.driver.maxResultSize\":\"2000M\"} \n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'], \n",
        "                           gpu=True, \n",
        "                           params=params)\n",
        "\n",
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 3.3.1\n",
            "Spark NLP_JSL Version : 3.3.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4fc135e17700:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f71d0622d50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXs5Z5wgMe4o"
      },
      "source": [
        "# Import auxiliary libraries\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.feature import StringIndexer, CountVectorizer, HashingTF, IDF\n",
        "from pyspark.sql import functions as F\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier as skl_RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression as skl_LogisticRegression"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgkQ9G5VkUtx"
      },
      "source": [
        "### Reading in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0LXuIzSOek1"
      },
      "source": [
        "# Get datasets\n",
        "! wget -q https://raw.githubusercontent.com/socd06/medical-nlp/master/data/train.csv\n",
        "! wget -q https://raw.githubusercontent.com/socd06/medical-nlp/master/data/test.csv"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS4FXeWOfRy1"
      },
      "source": [
        "labelDict = {'1':'Surgery', '2':'Medical Records', '3':'Internal Medicine', '4':'Other'}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhNrz4j1AUHf"
      },
      "source": [
        "trainDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"train.csv\") \\\n",
        "      .replace(labelDict, subset=['label'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL-1j29m12tc",
        "outputId": "864df0dc-145c-4a05-e0bc-18d8363687c8"
      },
      "source": [
        "trainDataset.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTmouQW7GRM2",
        "outputId": "602228a4-c64a-4cdf-b2c7-120d6a1a98e1"
      },
      "source": [
        "trainDataset.show(10, truncate=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------------------------+--------------------------------------------------+\n",
            "|            label|                          description|                                              text|\n",
            "+-----------------+-------------------------------------+--------------------------------------------------+\n",
            "|  Medical Records|                         2-D Doppler |2-D STUDY,1. Mild aortic stenosis, widely calci...|\n",
            "|          Surgery|                         Gastroscopy |PREOPERATIVE DIAGNOSES: , Dysphagia and esophag...|\n",
            "|  Medical Records|       Three-Week Postpartum Checkup |CHIEF COMPLAINT:,  The patient comes for three-...|\n",
            "|          Surgery|             Radiofrequency Ablation |PROCEDURE: , Bilateral L5, S1, S2, and S3 radio...|\n",
            "|  Medical Records|               Discharge Summary - 3 |DISCHARGE DIAGNOSES:,1. Chronic obstructive pul...|\n",
            "|Internal Medicine| Heart Catheterization & Angiography |INDICATION:,  Coronary artery disease, severe a...|\n",
            "|  Medical Records|                Gen Med Consult - 21 |SUBJECTIVE:,  The patient is a 2-year-old littl...|\n",
            "|  Medical Records|   Neuropsychological Evaluation - 1 |REASON FOR EVALUATION: , The patient is a 37-ye...|\n",
            "|  Medical Records| MRI Brain -  Meningioma (Olfactory) |CC:, Progressive visual loss.,HX:, 76 y/o male ...|\n",
            "|Internal Medicine|                      Knee Injection |The patient was told that the injection may cau...|\n",
            "+-----------------+-------------------------------------+--------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXLLrcaBPDkI",
        "outputId": "f6ab46d8-0e84-405b-b696-cbca4347b549"
      },
      "source": [
        "trainDataset.groupBy(\"label\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|            label|count|\n",
            "+-----------------+-----+\n",
            "|          Surgery| 1442|\n",
            "|  Medical Records| 1126|\n",
            "|Internal Medicine| 1040|\n",
            "|            Other|  891|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tY3LrNGStPm"
      },
      "source": [
        "testDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"test.csv\") \\\n",
        "      .replace(labelDict, subset=['label'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE7gLVSwStVd",
        "outputId": "b8407932-9bbe-49ab-8ccc-b559bfa743f5"
      },
      "source": [
        "testDataset.show(10, truncate=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------------------------------+--------------------------------------------------+\n",
            "|            label|                                description|                                              text|\n",
            "+-----------------+-------------------------------------------+--------------------------------------------------+\n",
            "|  Medical Records|      Hemiarthroplasty - Discharge Summary |ADMISSION DIAGNOSES:  ,Fracture of the right fe...|\n",
            "|          Surgery|                        Plantar Fasciotomy |PREOPERATIVE DIAGNOSIS:,  Plantar fascitis, lef...|\n",
            "|  Medical Records|      Hysterectomy - Discharge Summary - 2 |ADMISSION DIAGNOSIS: , Microinvasive carcinoma ...|\n",
            "|            Other|        Total Knee Arthoplasty - Right - 1 |PREOPERATIVE DIAGNOSIS:,  Severe degenerative j...|\n",
            "|          Surgery|         Breast Radiation Therapy Followup |DIAGNOSIS: , Left breast adenocarcinoma stage T...|\n",
            "|            Other|                         Hamstring Release |PREOPERATIVE DIAGNOSIS: , Autism with bilateral...|\n",
            "|            Other| Ruptured Globe Repair - Sclera and Limbus |PREOPERATIVE DIAGNOSIS: , Ruptured globe with u...|\n",
            "|Internal Medicine|                   Mediastinal Exploration |TITLE OF OPERATION:,  Mediastinal exploration a...|\n",
            "|Internal Medicine|                   Normal ROS Template - 5 |REVIEW OF SYSTEMS,GENERAL:  Negative weakness, ...|\n",
            "|          Surgery|          Total Abdominal Hysterectomy - 2 |PREOPERATIVE DIAGNOSES:,1.  Enlarged fibroid ut...|\n",
            "+-----------------+-------------------------------------------+--------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15gV5E7lSuEk",
        "outputId": "ddd6c136-6052-4669-a059-4a79d77a212f"
      },
      "source": [
        "testDataset.groupBy(\"label\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|            label|count|\n",
            "+-----------------+-----+\n",
            "|          Surgery|  198|\n",
            "|Internal Medicine|  109|\n",
            "|  Medical Records|  102|\n",
            "|            Other|   91|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1sGjg1ziO93"
      },
      "source": [
        "## DL Classifiers with Sentence Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJu03s0TUCLO"
      },
      "source": [
        "### DL Classification with Universal Sentence Encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9WK0cvhBsiP",
        "outputId": "49d9747f-c4f3-40c5-d2d5-158cf713e2a3"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setMaxEpochs(50)\\\n",
        "    .setLr(0.001)\\\n",
        "    .setBatchSize(8)\\\n",
        "    .setEnableOutputLogs(True)\n",
        "\n",
        "use_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        use,\n",
        "        classifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBQDo_0xBstz"
      },
      "source": [
        "use_pipelineModel = use_clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrDlYMlQbE8T",
        "outputId": "9e297902-3553-461a-ee56-6d9b5a7fc544"
      },
      "source": [
        "# Evaluate model on test set\n",
        "use_df = use_pipelineModel.transform(testDataset).select('label', 'text', 'class.result').toPandas()\n",
        "use_df['result'] = use_df['result'].apply(lambda x: str(x[0])).replace(labelDict)\n",
        "print(classification_report(use_df.label, use_df.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Internal Medicine       0.33      0.03      0.05       109\n",
            "  Medical Records       0.41      0.90      0.57       102\n",
            "            Other       0.00      0.00      0.00        91\n",
            "          Surgery       0.65      0.88      0.75       198\n",
            "\n",
            "         accuracy                           0.54       500\n",
            "        macro avg       0.35      0.45      0.34       500\n",
            "     weighted avg       0.41      0.54      0.42       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA6nNjob6d-U"
      },
      "source": [
        "### DL Classification with BERT Sentence Embeddings (Compact Version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xRqKnxhmHpr",
        "outputId": "c16516a0-48cf-4b66-c287-8105b223937a"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "bert_sent = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L8_512\")\\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setMaxEpochs(15)\\\n",
        "    .setLr(1e-4)\\\n",
        "    .setBatchSize(8)\\\n",
        "    .setEnableOutputLogs(True)\n",
        "\n",
        "bert_sent_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        bert_sent,\n",
        "        classifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sent_small_bert_L8_512 download started this may take some time.\n",
            "Approximate size to download 149.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k636WCQ9y7b"
      },
      "source": [
        "bert_sent_pipelineModel = bert_sent_clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWEShs7Z9zFS",
        "outputId": "7d8db255-83b1-4810-8e31-89d6a4f710a9"
      },
      "source": [
        "# Evaluate model on test set\n",
        "bert_sent_df = bert_sent_pipelineModel.transform(testDataset).select('label', 'text', 'class.result').toPandas()\n",
        "bert_sent_df['result'] = bert_sent_df['result'].apply(lambda x: str(x[0])).replace(labelDict)\n",
        "print(classification_report(bert_sent_df.label, bert_sent_df.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Internal Medicine       0.00      0.00      0.00       109\n",
            "  Medical Records       0.40      0.81      0.53       102\n",
            "            Other       0.00      0.00      0.00        91\n",
            "          Surgery       0.60      0.88      0.71       198\n",
            "\n",
            "         accuracy                           0.51       500\n",
            "        macro avg       0.25      0.42      0.31       500\n",
            "     weighted avg       0.32      0.51      0.39       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSutOPJiH5gU"
      },
      "source": [
        "### DL Classification with BioBERT (Clnical) Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h55_agOvH9yk",
        "outputId": "07059d51-0c21-4395-ada3-db87d4384db7"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "biobert_clin = BertSentenceEmbeddings.pretrained(\"sent_biobert_clinical_base_cased\", \"en\")\\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setMaxEpochs(200)\\\n",
        "    .setLr(3e-4)\\\n",
        "    .setBatchSize(8)\\\n",
        "    .setEnableOutputLogs(True)\n",
        "\n",
        "biobert_clin_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        biobert_clin,\n",
        "        classifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sent_biobert_clinical_base_cased download started this may take some time.\n",
            "Approximate size to download 386.6 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnYtRLHcJ0us"
      },
      "source": [
        "biobert_clin_pipelineModel = biobert_clin_clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPyyovvdnEma",
        "outputId": "6a8ae280-6f0f-4a14-da6d-8f7cc8dab307"
      },
      "source": [
        "log_file_name = os.listdir(\"/root/annotator_logs\")[2]\n",
        "\n",
        "with open(\"/root/annotator_logs/\"+log_file_name, \"r\") as log_file :\n",
        "    print(log_file.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started - epochs: 100 - learning_rate: 3.0E-4 - batch_size: 8 - training_examples: 4499 - classes: 4\n",
            "Epoch 0/100 - 1.66s - loss: 712.29126 - acc: 0.48658067 - batches: 563\n",
            "Epoch 1/100 - 1.53s - loss: 695.0834 - acc: 0.5099348 - batches: 563\n",
            "Epoch 2/100 - 1.53s - loss: 693.25885 - acc: 0.5139383 - batches: 563\n",
            "Epoch 3/100 - 1.58s - loss: 691.974 - acc: 0.5139383 - batches: 563\n",
            "Epoch 4/100 - 1.59s - loss: 690.9701 - acc: 0.5157177 - batches: 563\n",
            "Epoch 5/100 - 1.58s - loss: 690.14355 - acc: 0.51771945 - batches: 563\n",
            "Epoch 6/100 - 1.57s - loss: 689.4407 - acc: 0.5194988 - batches: 563\n",
            "Epoch 7/100 - 1.57s - loss: 688.8167 - acc: 0.5208334 - batches: 563\n",
            "Epoch 8/100 - 1.57s - loss: 688.26056 - acc: 0.5212782 - batches: 563\n",
            "Epoch 9/100 - 1.59s - loss: 687.7543 - acc: 0.52216786 - batches: 563\n",
            "Epoch 10/100 - 1.58s - loss: 687.2945 - acc: 0.5226127 - batches: 563\n",
            "Epoch 11/100 - 1.59s - loss: 686.87646 - acc: 0.5237248 - batches: 563\n",
            "Epoch 12/100 - 1.58s - loss: 686.4867 - acc: 0.5255042 - batches: 563\n",
            "Epoch 13/100 - 1.60s - loss: 686.12537 - acc: 0.5248369 - batches: 563\n",
            "Epoch 14/100 - 1.59s - loss: 685.788 - acc: 0.52617145 - batches: 563\n",
            "Epoch 15/100 - 1.57s - loss: 685.4723 - acc: 0.52683866 - batches: 563\n",
            "Epoch 16/100 - 1.67s - loss: 685.1784 - acc: 0.5266163 - batches: 563\n",
            "Epoch 17/100 - 1.58s - loss: 684.89624 - acc: 0.52750593 - batches: 563\n",
            "Epoch 18/100 - 1.58s - loss: 684.6356 - acc: 0.52750593 - batches: 563\n",
            "Epoch 19/100 - 1.57s - loss: 684.3841 - acc: 0.52750593 - batches: 563\n",
            "Epoch 20/100 - 1.57s - loss: 684.1481 - acc: 0.52861804 - batches: 563\n",
            "Epoch 21/100 - 1.57s - loss: 683.9223 - acc: 0.5288405 - batches: 563\n",
            "Epoch 22/100 - 1.58s - loss: 683.7045 - acc: 0.52950776 - batches: 563\n",
            "Epoch 23/100 - 1.57s - loss: 683.49554 - acc: 0.52950776 - batches: 563\n",
            "Epoch 24/100 - 1.58s - loss: 683.2971 - acc: 0.52906287 - batches: 563\n",
            "Epoch 25/100 - 1.58s - loss: 683.10394 - acc: 0.52906287 - batches: 563\n",
            "Epoch 26/100 - 1.57s - loss: 682.9172 - acc: 0.5288405 - batches: 563\n",
            "Epoch 27/100 - 1.59s - loss: 682.73706 - acc: 0.52906287 - batches: 563\n",
            "Epoch 28/100 - 1.59s - loss: 682.56586 - acc: 0.52950776 - batches: 563\n",
            "Epoch 29/100 - 1.59s - loss: 682.39966 - acc: 0.5299526 - batches: 563\n",
            "Epoch 30/100 - 1.59s - loss: 682.2395 - acc: 0.5303974 - batches: 563\n",
            "Epoch 31/100 - 1.59s - loss: 682.08514 - acc: 0.53084224 - batches: 563\n",
            "Epoch 32/100 - 1.58s - loss: 681.9343 - acc: 0.5303974 - batches: 563\n",
            "Epoch 33/100 - 1.59s - loss: 681.7886 - acc: 0.5299526 - batches: 563\n",
            "Epoch 34/100 - 1.58s - loss: 681.6487 - acc: 0.5310647 - batches: 563\n",
            "Epoch 35/100 - 1.58s - loss: 681.5139 - acc: 0.53084224 - batches: 563\n",
            "Epoch 36/100 - 1.58s - loss: 681.3838 - acc: 0.5303974 - batches: 563\n",
            "Epoch 37/100 - 1.57s - loss: 681.25433 - acc: 0.53084224 - batches: 563\n",
            "Epoch 38/100 - 1.58s - loss: 681.1283 - acc: 0.5310647 - batches: 563\n",
            "Epoch 39/100 - 1.58s - loss: 681.0062 - acc: 0.53084224 - batches: 563\n",
            "Epoch 40/100 - 1.58s - loss: 680.88806 - acc: 0.5306198 - batches: 563\n",
            "Epoch 41/100 - 1.56s - loss: 680.77313 - acc: 0.5310647 - batches: 563\n",
            "Epoch 42/100 - 1.57s - loss: 680.6618 - acc: 0.53084224 - batches: 563\n",
            "Epoch 43/100 - 1.80s - loss: 680.5542 - acc: 0.5312871 - batches: 563\n",
            "Epoch 44/100 - 1.57s - loss: 680.44836 - acc: 0.5315095 - batches: 563\n",
            "Epoch 45/100 - 1.59s - loss: 680.3459 - acc: 0.5321768 - batches: 563\n",
            "Epoch 46/100 - 1.58s - loss: 680.24585 - acc: 0.5321768 - batches: 563\n",
            "Epoch 47/100 - 1.58s - loss: 680.1487 - acc: 0.5323992 - batches: 563\n",
            "Epoch 48/100 - 1.59s - loss: 680.0535 - acc: 0.5317319 - batches: 563\n",
            "Epoch 49/100 - 1.58s - loss: 679.9583 - acc: 0.5317319 - batches: 563\n",
            "Epoch 50/100 - 1.58s - loss: 679.8668 - acc: 0.5317319 - batches: 563\n",
            "Epoch 51/100 - 1.58s - loss: 679.7767 - acc: 0.53195435 - batches: 563\n",
            "Epoch 52/100 - 1.59s - loss: 679.6874 - acc: 0.5323992 - batches: 563\n",
            "Epoch 53/100 - 1.57s - loss: 679.5992 - acc: 0.5323992 - batches: 563\n",
            "Epoch 54/100 - 1.59s - loss: 679.5137 - acc: 0.5323992 - batches: 563\n",
            "Epoch 55/100 - 1.57s - loss: 679.4301 - acc: 0.532844 - batches: 563\n",
            "Epoch 56/100 - 1.57s - loss: 679.3482 - acc: 0.532844 - batches: 563\n",
            "Epoch 57/100 - 1.57s - loss: 679.2684 - acc: 0.5323992 - batches: 563\n",
            "Epoch 58/100 - 1.56s - loss: 679.1893 - acc: 0.5323992 - batches: 563\n",
            "Epoch 59/100 - 1.58s - loss: 679.112 - acc: 0.5323992 - batches: 563\n",
            "Epoch 60/100 - 1.57s - loss: 679.03613 - acc: 0.532844 - batches: 563\n",
            "Epoch 61/100 - 1.57s - loss: 678.9623 - acc: 0.53306645 - batches: 563\n",
            "Epoch 62/100 - 1.57s - loss: 678.8892 - acc: 0.5332889 - batches: 563\n",
            "Epoch 63/100 - 1.58s - loss: 678.81805 - acc: 0.5326216 - batches: 563\n",
            "Epoch 64/100 - 1.68s - loss: 678.7485 - acc: 0.5326216 - batches: 563\n",
            "Epoch 65/100 - 1.63s - loss: 678.6791 - acc: 0.53306645 - batches: 563\n",
            "Epoch 66/100 - 1.59s - loss: 678.6099 - acc: 0.5335113 - batches: 563\n",
            "Epoch 67/100 - 1.58s - loss: 678.54315 - acc: 0.5339561 - batches: 563\n",
            "Epoch 68/100 - 1.58s - loss: 678.4775 - acc: 0.5339561 - batches: 563\n",
            "Epoch 69/100 - 1.56s - loss: 678.4123 - acc: 0.5339561 - batches: 563\n",
            "Epoch 70/100 - 1.58s - loss: 678.3477 - acc: 0.5337337 - batches: 563\n",
            "Epoch 71/100 - 1.58s - loss: 678.28485 - acc: 0.5337337 - batches: 563\n",
            "Epoch 72/100 - 1.57s - loss: 678.22205 - acc: 0.53440094 - batches: 563\n",
            "Epoch 73/100 - 1.59s - loss: 678.1606 - acc: 0.53529066 - batches: 563\n",
            "Epoch 74/100 - 1.57s - loss: 678.1006 - acc: 0.5350682 - batches: 563\n",
            "Epoch 75/100 - 1.58s - loss: 678.041 - acc: 0.53529066 - batches: 563\n",
            "Epoch 76/100 - 1.58s - loss: 677.9818 - acc: 0.53595793 - batches: 563\n",
            "Epoch 77/100 - 1.59s - loss: 677.9235 - acc: 0.53595793 - batches: 563\n",
            "Epoch 78/100 - 1.58s - loss: 677.86707 - acc: 0.53551304 - batches: 563\n",
            "Epoch 79/100 - 1.59s - loss: 677.81195 - acc: 0.53551304 - batches: 563\n",
            "Epoch 80/100 - 1.59s - loss: 677.75555 - acc: 0.53529066 - batches: 563\n",
            "Epoch 81/100 - 1.56s - loss: 677.7019 - acc: 0.53529066 - batches: 563\n",
            "Epoch 82/100 - 1.58s - loss: 677.6479 - acc: 0.53529066 - batches: 563\n",
            "Epoch 83/100 - 1.57s - loss: 677.5944 - acc: 0.53529066 - batches: 563\n",
            "Epoch 84/100 - 1.57s - loss: 677.54266 - acc: 0.53529066 - batches: 563\n",
            "Epoch 85/100 - 1.59s - loss: 677.4903 - acc: 0.53551304 - batches: 563\n",
            "Epoch 86/100 - 1.58s - loss: 677.4385 - acc: 0.5357355 - batches: 563\n",
            "Epoch 87/100 - 1.57s - loss: 677.3881 - acc: 0.5357355 - batches: 563\n",
            "Epoch 88/100 - 1.58s - loss: 677.3379 - acc: 0.53551304 - batches: 563\n",
            "Epoch 89/100 - 1.57s - loss: 677.288 - acc: 0.53529066 - batches: 563\n",
            "Epoch 90/100 - 1.56s - loss: 677.2388 - acc: 0.53551304 - batches: 563\n",
            "Epoch 91/100 - 1.58s - loss: 677.1899 - acc: 0.53551304 - batches: 563\n",
            "Epoch 92/100 - 1.57s - loss: 677.1419 - acc: 0.5357355 - batches: 563\n",
            "Epoch 93/100 - 1.85s - loss: 677.09424 - acc: 0.53529066 - batches: 563\n",
            "Epoch 94/100 - 1.58s - loss: 677.04785 - acc: 0.53551304 - batches: 563\n",
            "Epoch 95/100 - 1.60s - loss: 677.0015 - acc: 0.53551304 - batches: 563\n",
            "Epoch 96/100 - 1.58s - loss: 676.95544 - acc: 0.53595793 - batches: 563\n",
            "Epoch 97/100 - 1.57s - loss: 676.91077 - acc: 0.5357355 - batches: 563\n",
            "Epoch 98/100 - 1.61s - loss: 676.8664 - acc: 0.53595793 - batches: 563\n",
            "Epoch 99/100 - 1.60s - loss: 676.8223 - acc: 0.53595793 - batches: 563\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0llyyGuCJ0us",
        "outputId": "cb411476-2279-47f1-f469-d8bb6ec8beb0"
      },
      "source": [
        "# Evaluate model on test set\n",
        "biobert_clin_df = biobert_clin_pipelineModel.transform(testDataset).select('label', 'text', 'class.result').toPandas()\n",
        "biobert_clin_df['result'] = biobert_clin_df['result'].apply(lambda x: str(x[0])).replace(labelDict)\n",
        "print(classification_report(biobert_clin_df.label, biobert_clin_df.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Internal Medicine       0.42      0.16      0.23       109\n",
            "  Medical Records       0.44      0.85      0.58       102\n",
            "            Other       0.00      0.00      0.00        91\n",
            "          Surgery       0.64      0.85      0.73       198\n",
            "\n",
            "         accuracy                           0.54       500\n",
            "        macro avg       0.38      0.46      0.38       500\n",
            "     weighted avg       0.44      0.54      0.46       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbQzBjfH-33"
      },
      "source": [
        "### DL Classification with BioBERT (MedNLI) Sentence Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYGbXj09IBT8",
        "outputId": "04f26e79-bb14-4791-b0db-be88de640fe2"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "biobert_med = BertSentenceEmbeddings.pretrained(\"sbiobert_base_cased_mli\",\"en\",\"clinical/models\")\\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setMaxEpochs(50)\\\n",
        "    .setLr(3e-4)\\\n",
        "    .setBatchSize(8)\\\n",
        "    .setEnableOutputLogs(True)\n",
        "\n",
        "biobert_med_clf_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        biobert_med,\n",
        "        classifierdl\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sbiobert_base_cased_mli download started this may take some time.\n",
            "Approximate size to download 384.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5VbHeoGJ_qT"
      },
      "source": [
        "biobert_med_pipelineModel = biobert_med_clf_pipeline.fit(trainDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH3CtC44J_qT",
        "outputId": "1781d416-579a-424e-ca97-7da3aa1b2639"
      },
      "source": [
        "# Evaluate model on test set\n",
        "biobert_med_df = biobert_med_pipelineModel.transform(testDataset).select('label', 'text', 'class.result').toPandas()\n",
        "biobert_med_df['result'] = biobert_med_df['result'].apply(lambda x: str(x[0])).replace(labelDict)\n",
        "print(classification_report(biobert_med_df.label, biobert_med_df.result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Internal Medicine       0.36      0.24      0.29       109\n",
            "  Medical Records       0.40      0.68      0.50       102\n",
            "            Other       0.00      0.00      0.00        91\n",
            "          Surgery       0.62      0.80      0.70       198\n",
            "\n",
            "         accuracy                           0.51       500\n",
            "        macro avg       0.34      0.43      0.37       500\n",
            "     weighted avg       0.41      0.51      0.44       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFqQnRAEjUnu"
      },
      "source": [
        "## ML Classifiers with Sentence Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMSeY7c2coT"
      },
      "source": [
        "### Logistic Regression with Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRvZjR6Y2e4x"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "use = UniversalSentenceEncoder.pretrained()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "embeddings_finisher = EmbeddingsFinisher() \\\n",
        "      .setInputCols([\"sentence_embeddings\"]) \\\n",
        "      .setOutputCols([\"finished_embeddings\"]) \\\n",
        "      .setOutputAsVector(True)\\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\")\n",
        "\n",
        "ml_pipeline = Pipeline(\n",
        "      stages=[\n",
        "        document,\n",
        "        use,\n",
        "        embeddings_finisher,\n",
        "        label_stringIdx]\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--lJx2Je22FF"
      },
      "source": [
        "# Fit pipeline to train and test sets\n",
        "ml_model = ml_pipeline.fit(trainDataset)\n",
        "ml_train = ml_model.transform(trainDataset)\n",
        "ml_test = ml_model.transform(testDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yDcN2El3jl-"
      },
      "source": [
        "# Explode sentence embeddings for train and test sets\n",
        "ml_train = ml_train.withColumn(\"features\", explode(ml_train.finished_embeddings))\n",
        "ml_test = ml_test.withColumn(\"features\", explode(ml_test.finished_embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72EWTF-431cP"
      },
      "source": [
        "# Fit logreg\n",
        "lr = LogisticRegression(labelCol=\"class\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(ml_train)\n",
        "\n",
        "# Get test set preds\n",
        "lrPredictions = lrModel.transform(ml_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHRuxNj_4S62",
        "outputId": "cf57217e-fc40-4993-d697-78e3f1859445"
      },
      "source": [
        "# Evaluate performance\n",
        "logreg_df = lrPredictions.select('text','label','class','prediction').toPandas()\n",
        "print(classification_report(logreg_df[\"class\"], logreg_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.70      0.68       198\n",
            "         1.0       0.41      0.53      0.46       102\n",
            "         2.0       0.39      0.28      0.32       109\n",
            "         3.0       0.40      0.36      0.38        91\n",
            "\n",
            "    accuracy                           0.51       500\n",
            "   macro avg       0.47      0.47      0.46       500\n",
            "weighted avg       0.50      0.51      0.50       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTAImbASBT-L"
      },
      "source": [
        "### Random Forest with Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcL2Ta9bBWRF"
      },
      "source": [
        "rf = RandomForestClassifier(labelCol=\"class\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)\n",
        "\n",
        "# Train model with Training Data, get predictions on test set\n",
        "rfModel = rf.fit(ml_train)\n",
        "predictions_rf = rfModel.transform(ml_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxI0Y7HrdpuQ",
        "outputId": "45148000-23bd-4de6-8b96-ebb19c13382a"
      },
      "source": [
        "# Evaluate performance\n",
        "rf_df = predictions_rf.select(\"class\", \"prediction\").toPandas()\n",
        "print(classification_report(rf_df[\"class\"], rf_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.63      0.87      0.73       198\n",
            "         1.0       0.42      0.85      0.56       102\n",
            "         2.0       0.14      0.01      0.02       109\n",
            "         3.0       0.50      0.08      0.13        91\n",
            "\n",
            "    accuracy                           0.54       500\n",
            "   macro avg       0.42      0.45      0.36       500\n",
            "weighted avg       0.46      0.54      0.43       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWnTJGB2APJP"
      },
      "source": [
        "## ML Classifiers with Feature Vectorization Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir9O7ty6AYO0"
      },
      "source": [
        "### Logistic Regression with CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQgoftVwAXIC"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "      \n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "      .setInputCols([\"cleanTokens\"]) \\\n",
        "      .setOutputCol(\"stem\")\n",
        "\n",
        "finisher = Finisher() \\\n",
        "      .setInputCols([\"stem\"]) \\\n",
        "      .setOutputCols([\"token_features\"]) \\\n",
        "      .setOutputAsArray(True) \\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "countVectors = CountVectorizer(inputCol=\"token_features\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\")\n",
        "\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            countVectors,\n",
        "            label_stringIdx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rd0BvVEBnJS"
      },
      "source": [
        "# Fit pipeline to train and test set\n",
        "nlp_model = nlp_pipeline.fit(trainDataset)\n",
        "countvec_train = nlp_model.transform(trainDataset)\n",
        "countvec_test = nlp_model.transform(testDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI1tTTy2Bm9Y",
        "outputId": "8b96b387-5a6e-481f-cdc2-98ffab38260e"
      },
      "source": [
        "countvec_train.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- label: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- normalized: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- cleanTokens: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- stem: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token_features: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- class: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcsgl1onEtHQ"
      },
      "source": [
        "# Fit logreg\n",
        "lr = LogisticRegression(labelCol=\"class\", maxIter=10, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(countvec_train)\n",
        "\n",
        "# Get test set preds\n",
        "lrPredictions = lrModel.transform(countvec_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck9yaatTEtHQ",
        "outputId": "be2e216c-5555-4b38-a477-4d61996cfad8"
      },
      "source": [
        "# Evaluate performance\n",
        "logreg_df = lrPredictions.select('text','label','class','prediction').toPandas()\n",
        "print(classification_report(logreg_df[\"class\"], logreg_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.60      0.63      0.61       198\n",
            "         1.0       0.23      0.26      0.24       102\n",
            "         2.0       0.23      0.19      0.21       109\n",
            "         3.0       0.23      0.21      0.22        91\n",
            "\n",
            "    accuracy                           0.38       500\n",
            "   macro avg       0.32      0.32      0.32       500\n",
            "weighted avg       0.38      0.38      0.38       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED81TpQCBC77"
      },
      "source": [
        "### Logistic Regression with TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ebRhYCBoKq"
      },
      "source": [
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "nlp_pipeline_tf = Pipeline(\n",
        "    stages=[document, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            label_stringIdx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJUbN7N7BoIf"
      },
      "source": [
        "# Fit pipeline to train and test set\n",
        "tfidf_model = nlp_pipeline_tf.fit(trainDataset)\n",
        "tfidf_train = tfidf_model.transform(trainDataset)\n",
        "tfidf_test = tfidf_model.transform(testDataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RgDCXM0Bpwp"
      },
      "source": [
        "# Fit logreg\n",
        "lr = LogisticRegression(labelCol=\"class\", maxIter=10, regParam=0.3, elasticNetParam=0)\n",
        "lrModel_tf = lr.fit(tfidf_train)\n",
        "\n",
        "# Get test set preds\n",
        "lrPredictions_tf = lrModel_tf.transform(tfidf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPCQbemBqnn",
        "outputId": "6b02500f-7438-48f1-a03d-8e7da5adfa04"
      },
      "source": [
        "# Evaluate performance\n",
        "logreg_tf_df = lrPredictions_tf.select('class','prediction').toPandas()\n",
        "print(classification_report(logreg_tf_df[\"class\"], logreg_tf_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.60      0.59       198\n",
            "         1.0       0.25      0.31      0.28       102\n",
            "         2.0       0.16      0.12      0.14       109\n",
            "         3.0       0.20      0.20      0.20        91\n",
            "\n",
            "    accuracy                           0.36       500\n",
            "   macro avg       0.30      0.31      0.30       500\n",
            "weighted avg       0.35      0.36      0.36       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcG8DwQxbyVQ"
      },
      "source": [
        "### Random Forest with CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP381v3ub1k0"
      },
      "source": [
        "rf = RandomForestClassifier(labelCol=\"class\", \\\n",
        "                            featuresCol=\"features\", \\\n",
        "                            numTrees = 100, \\\n",
        "                            maxDepth = 4, \\\n",
        "                            maxBins = 32)\n",
        "\n",
        "# Train model and get predictions on test set\n",
        "rfModel_cv = rf.fit(countvec_train)\n",
        "predictions_rf_cv = rfModel_cv.transform(countvec_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6_FLDG0b7u3",
        "outputId": "8daf5db5-344b-4baf-8b5e-866d789faff3"
      },
      "source": [
        "# Evaluate performance\n",
        "rf_cv_df = predictions_rf_cv.select(\"class\", \"prediction\").toPandas()\n",
        "print(classification_report(rf_cv_df[\"class\"], rf_cv_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.86      0.75       198\n",
            "         1.0       0.41      0.98      0.58       102\n",
            "         2.0       1.00      0.01      0.02       109\n",
            "         3.0       0.00      0.00      0.00        91\n",
            "\n",
            "    accuracy                           0.54       500\n",
            "   macro avg       0.52      0.46      0.34       500\n",
            "weighted avg       0.56      0.54      0.42       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAfirCbhBVh5"
      },
      "source": [
        "### Random Forest with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySBauV9GBXpO"
      },
      "source": [
        "# Train model and get predictions on test set\n",
        "rfModel_tf = rf.fit(tfidf_train)\n",
        "predictions_rf_tf = rfModel_tf.transform(tfidf_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESdOD7WkBpdX",
        "outputId": "bff62853-63c1-47c2-e92d-b9600fd0ab6a"
      },
      "source": [
        "# Evaluate performance\n",
        "rf_tf_df = predictions_rf_tf.select(\"class\", \"prediction\").toPandas()\n",
        "print(classification_report(rf_tf_df[\"class\"], rf_tf_df.prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.65      0.87      0.75       198\n",
            "         1.0       0.40      0.92      0.56       102\n",
            "         2.0       0.00      0.00      0.00       109\n",
            "         3.0       0.00      0.00      0.00        91\n",
            "\n",
            "    accuracy                           0.53       500\n",
            "   macro avg       0.26      0.45      0.33       500\n",
            "weighted avg       0.34      0.53      0.41       500\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6260gTTTygq4"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK2G4dCyzqQZ"
      },
      "source": [
        "### Featurizer Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuGq0cqjwJCu"
      },
      "source": [
        "# Function to get counts of each assertion\n",
        "def get_assertion_stats(assertion):\n",
        "    ass_list=[]\n",
        "    for s in assertion:\n",
        "        ass_list.extend(s)\n",
        "    x = dict(Counter(assertion))\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2J-n6YPt14F"
      },
      "source": [
        "# Function to extract features out of df\n",
        "def create_features(spark_df, pipeline):\n",
        "    if pipeline == 'full':\n",
        "        # Exploding entities and icd codes\n",
        "        pandas_df = spark_df.select('id','label','class','assertion','source',\n",
        "                  F.explode(F.arrays_zip('clinical_ner_chunk.result',\"clinical_ner_chunk.metadata\",\n",
        "                                         'bio_ner_chunk.result',\"bio_ner_chunk.metadata\",\n",
        "                                         'posology_ner_chunk.result',\"posology_ner_chunk.metadata\",\n",
        "                                         'risk_ner_chunk.result',\"risk_ner_chunk.metadata\",\n",
        "                                         'jsl_ner_chunk.result',\"jsl_ner_chunk.metadata\",\n",
        "                                         'icd10cm_code.result','icd10pcs_code.result')).alias(\"cols\")) \\\n",
        "              .select('id','label','class','source',\n",
        "                      F.expr(\"assertion.result\").alias(\"assertion\"),\n",
        "                      F.expr(\"cols['0']\").alias(\"clinical_token\"),\n",
        "                      F.expr(\"cols['1'].entity\").alias(\"clinical_entity\"),\n",
        "                      F.expr(\"cols['2']\").alias(\"bionlp_token\"),\n",
        "                      F.expr(\"cols['3'].entity\").alias(\"bionlp_entity\"),\n",
        "                      F.expr(\"cols['4']\").alias(\"posology_token\"),\n",
        "                      F.expr(\"cols['5'].entity\").alias(\"posology_entity\"),\n",
        "                      F.expr(\"cols['6']\").alias(\"risk_token\"),\n",
        "                      F.expr(\"cols['7'].entity\").alias(\"risk_entity\"),\n",
        "                      F.expr(\"cols['8']\").alias(\"jsl_token\"),\n",
        "                      F.expr(\"cols['9'].entity\").alias(\"jsl_entity\"),\n",
        "                      F.expr(\"cols['10']\").alias(\"icd10cm_code\"),\n",
        "                      F.expr(\"cols['11']\").alias(\"icd10pcs_code\")).toPandas()\n",
        "\n",
        "        pids = pandas_df['id'].unique()\n",
        "\n",
        "        stats_list=[]\n",
        "\n",
        "        for i in pids:\n",
        "            # Getting counts of each entity, icd code and assertion status\n",
        "            temp_df = pandas_df[pandas_df.id==i]\n",
        "            temp_dict = temp_df['clinical_entity'].value_counts().to_dict()\n",
        "            temp_dict.update(temp_df['bionlp_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['posology_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['jsl_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['risk_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['icd10cm_code'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['icd10pcs_code'].value_counts().to_dict())\n",
        "            adf = temp_df['assertion'].apply(lambda x: get_assertion_stats(x)).value_counts().reset_index()\n",
        "\n",
        "            dic = {'present':0,\n",
        "            'absent':0,\n",
        "            'associated_with_someone_else':0}\n",
        "\n",
        "            for j, row in adf.iterrows():\n",
        "                try:\n",
        "                    k = list(row['index'].keys())[0]\n",
        "                    dic[k]= dic[k]+row['index'][k]*row['assertion']\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Finalizing dictionary\n",
        "            temp_dict.update(dic)\n",
        "            temp_dict['id']=i\n",
        "            stats_list.append(temp_dict)\n",
        "\n",
        "    elif pipeline == 'no_resolver':\n",
        "        # Exploding entities and icd codes\n",
        "        pandas_df = spark_df.select('id','label','class','assertion','source',\n",
        "                  F.explode(F.arrays_zip('clinical_ner_chunk.result',\"clinical_ner_chunk.metadata\",\n",
        "                                         'bio_ner_chunk.result',\"bio_ner_chunk.metadata\",\n",
        "                                         'posology_ner_chunk.result',\"posology_ner_chunk.metadata\",\n",
        "                                         'risk_ner_chunk.result',\"risk_ner_chunk.metadata\",\n",
        "                                         'jsl_ner_chunk.result',\"jsl_ner_chunk.metadata\")).alias(\"cols\")) \\\n",
        "              .select('id','label','class','source',\n",
        "                      F.expr(\"assertion.result\").alias(\"assertion\"),\n",
        "                      F.expr(\"cols['0']\").alias(\"clinical_token\"),\n",
        "                      F.expr(\"cols['1'].entity\").alias(\"clinical_entity\"),\n",
        "                      F.expr(\"cols['2']\").alias(\"bionlp_token\"),\n",
        "                      F.expr(\"cols['3'].entity\").alias(\"bionlp_entity\"),\n",
        "                      F.expr(\"cols['4']\").alias(\"posology_token\"),\n",
        "                      F.expr(\"cols['5'].entity\").alias(\"posology_entity\"),\n",
        "                      F.expr(\"cols['6']\").alias(\"risk_token\"),\n",
        "                      F.expr(\"cols['7'].entity\").alias(\"risk_entity\"),\n",
        "                      F.expr(\"cols['8']\").alias(\"jsl_token\"),\n",
        "                      F.expr(\"cols['9'].entity\").alias(\"jsl_entity\")).toPandas()\n",
        "\n",
        "        pids = pandas_df['id'].unique()\n",
        "\n",
        "        stats_list=[]\n",
        "\n",
        "        for i in pids:\n",
        "            # Getting counts of each entity, icd code and assertion status\n",
        "            temp_df = pandas_df[pandas_df.id==i]\n",
        "            temp_dict = temp_df['clinical_entity'].value_counts().to_dict()\n",
        "            temp_dict.update(temp_df['bionlp_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['posology_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['jsl_entity'].value_counts().to_dict())\n",
        "            temp_dict.update(temp_df['risk_entity'].value_counts().to_dict())\n",
        "            adf = temp_df['assertion'].apply(lambda x: get_assertion_stats(x)).value_counts().reset_index()\n",
        "\n",
        "            dic = {'present':0,\n",
        "            'absent':0,\n",
        "            'associated_with_someone_else':0}\n",
        "\n",
        "            for j, row in adf.iterrows():\n",
        "                try:\n",
        "                    k = list(row['index'].keys())[0]\n",
        "                    dic[k]= dic[k]+row['index'][k]*row['assertion']\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Finalizing dictionary\n",
        "            temp_dict.update(dic)\n",
        "            temp_dict['id']=i\n",
        "            stats_list.append(temp_dict)\n",
        "\n",
        "    # Converting to dataframe\n",
        "    stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "    # Renaming cols\n",
        "    stats_df.columns = ['entity_{}'.format(c) for c in stats_df.columns]\n",
        "      \n",
        "    # Merging counts with unique ids and corresponding labels\n",
        "    model_df = pandas_df[['id','label','class', 'source']].drop_duplicates().merge(stats_df, left_on='id', right_on='entity_id').fillna(0)\n",
        "\n",
        "    return model_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kCp-v8J4x8h"
      },
      "source": [
        "# Create monotonically increasing ids \n",
        "trainDataset = trainDataset.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "testDataset = testDataset.withColumn(\"id\", F.monotonically_increasing_id())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VRPp-72zlXr"
      },
      "source": [
        "### Full Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6oGlZKp44Xg"
      },
      "source": [
        "#### Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvK_EvVpyh4B",
        "outputId": "e8f8f4f1-e787-49e5-caea-b774fbae4a07"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n",
        "        .setInputCols([\"document\"])\\\n",
        "        .setOutputCol(\"sentence\")\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('tok_checked')\n",
        "\n",
        "embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models')\\\n",
        "    .setInputCols([\"sentence\", \"tok_checked\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# Detect clinical problems, tests and treatments\n",
        "clinical_ner = MedicalNerModel.pretrained(\"ner_clinical_large\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"clinical_ner\")\n",
        "\n",
        "clinical_converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"clinical_ner\"])\\\n",
        "  .setOutputCol(\"clinical_ner_chunk\")\n",
        "\n",
        "# Detect clinical entities\n",
        "jsl_ner = MedicalNerModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"jsl_ner\")\n",
        "\n",
        "jsl_converter = NerConverter() \\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"jsl_ner\"]) \\\n",
        "  .setOutputCol(\"jsl_ner_chunk\")\n",
        "\n",
        "# Detect cancer genetics\n",
        "bio_ner = MedicalNerModel.pretrained(\"ner_bionlp\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"bio_ner\")\n",
        "\n",
        "bio_converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"bio_ner\"])\\\n",
        "  .setOutputCol(\"bio_ner_chunk\")\n",
        "\n",
        "# Detect drugs and posology entities\n",
        "posology_ner = MedicalNerModel.pretrained(\"ner_posology\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"tok_checked\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"posology_ner\")\n",
        "\n",
        "posology_converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"posology_ner\"])\\\n",
        "  .setOutputCol(\"posology_ner_chunk\")    \n",
        "\n",
        "# Detect risk factors\n",
        "risk_ner = MedicalNerModel.pretrained(\"ner_risk_factors\", \"en\", \"clinical/models\") \\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"embeddings\"]) \\\n",
        "  .setOutputCol(\"risk_ner\")\n",
        "\n",
        "risk_converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\", \"tok_checked\", \"risk_ner\"])\\\n",
        "  .setOutputCol(\"risk_ner_chunk\")    \n",
        "\n",
        "# Detect assertion status of risk entities\n",
        "risk_assertion = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"risk_ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "\n",
        "# Take clinical NER chunk, convert to doc\n",
        "chunk_doc = Chunk2Doc()\\\n",
        "      .setInputCols(\"clinical_ner_chunk\")\\\n",
        "      .setOutputCol(\"ner_chunk_doc\")\n",
        "\n",
        "# Get sentence-chunk Bert embeddings\n",
        "sentence_chunk_embeddings = BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
        "      .setInputCols([\"ner_chunk_doc\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# Sentence Entity Resolver for billable ICD10-CM HCC codes    \n",
        "icd_cm_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_icd10cm_augmented\",\"en\", \"clinical/models\") \\\n",
        "     .setInputCols([\"clinical_ner_chunk\", \"sentence_embeddings\"]) \\\n",
        "     .setOutputCol(\"icd10cm_code\")\\\n",
        "     .setDistanceFunction(\"EUCLIDEAN\")\n",
        "\n",
        "# Sentence Entity Resolver for ICD10-PCS\n",
        "icd_pcs_resolver = SentenceEntityResolverModel.pretrained(\"sbiobertresolve_icd10pcs\",\"en\", \"clinical/models\") \\\n",
        "     .setInputCols([\"clinical_ner_chunk\", \"sentence_embeddings\"]) \\\n",
        "     .setOutputCol(\"icd10pcs_code\")\\\n",
        "     .setDistanceFunction(\"EUCLIDEAN\")\n",
        "\n",
        "# Get numeric class\n",
        "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"class\", handleInvalid=\"keep\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl_healthcare download started this may take some time.\n",
            "Approximate size to download 367.3 KB\n",
            "[OK!]\n",
            "embeddings_clinical download started this may take some time.\n",
            "Approximate size to download 1.6 GB\n",
            "[OK!]\n",
            "ner_clinical_large download started this may take some time.\n",
            "Approximate size to download 13.9 MB\n",
            "[OK!]\n",
            "ner_jsl download started this may take some time.\n",
            "Approximate size to download 14.5 MB\n",
            "[OK!]\n",
            "ner_bionlp download started this may take some time.\n",
            "Approximate size to download 13.9 MB\n",
            "[OK!]\n",
            "ner_posology download started this may take some time.\n",
            "Approximate size to download 13.8 MB\n",
            "[OK!]\n",
            "ner_risk_factors download started this may take some time.\n",
            "Approximate size to download 13.9 MB\n",
            "[OK!]\n",
            "assertion_dl download started this may take some time.\n",
            "Approximate size to download 1.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev9JrnoR_kK9"
      },
      "source": [
        "full_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentenceDetector,\n",
        "        token,\n",
        "        embeddings,\n",
        "        clinical_ner,\n",
        "        clinical_converter,\n",
        "        jsl_ner,\n",
        "        jsl_converter,\n",
        "        bio_ner,\n",
        "        bio_converter,\n",
        "        posology_ner,\n",
        "        posology_converter,\n",
        "        risk_ner,\n",
        "        risk_converter,\n",
        "        risk_assertion,\n",
        "        chunk_doc,\n",
        "        sentence_chunk_embeddings,\n",
        "        icd_cm_resolver,\n",
        "        icd_pcs_resolver,\n",
        "        label_stringIdx\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7snzOamwGhcu"
      },
      "source": [
        "# Save unfit pipeline to disk\n",
        "full_pipeline.save(\"/content/full_pipeline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhHuTBYC9tLD"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/full_pipeline', 'zip', '/content/full_pipeline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwc4ls29L3B1"
      },
      "source": [
        "# Load unfit pipeline\n",
        "full_pipeline = PipelineModel.load(\"/content/full_pipeline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZOSmm4L4-DK"
      },
      "source": [
        "#### Fit Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giMK9lNGC4HF"
      },
      "source": [
        "# Limit dataset, add train/test differentiator\n",
        "limit_train = trainDataset.limit(500).withColumn(\"source\", lit(\"train\"))\n",
        "limit_test = testDataset.limit(500).withColumn(\"source\", lit(\"test\"))\n",
        "\n",
        "# Concat dataframes\n",
        "full_unionDF = limit_train.union(limit_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_d-V_OPrh5O"
      },
      "source": [
        "# Fit pipeline\n",
        "full_feature_model = full_pipeline.fit(full_unionDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuqxWr1esMkv"
      },
      "source": [
        "# Transform train and test set\n",
        "full_df = full_feature_model.transform(full_unionDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3_FE8r32Yqm",
        "outputId": "1d03691b-93d3-471e-928a-94dd3fb4885e"
      },
      "source": [
        "full_df.show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|          label|  description|                text| id|            document|            sentence|         tok_checked|          embeddings|        clinical_ner|  clinical_ner_chunk|             jsl_ner|       jsl_ner_chunk|             bio_ner|       bio_ner_chunk|        posology_ner|  posology_ner_chunk|            risk_ner|risk_ner_chunk|assertion|       ner_chunk_doc| sentence_embeddings|        icd10cm_code|       icd10pcs_code|class|\n",
            "+---------------+-------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|Medical Records| 2-D Doppler |2-D STUDY,1. Mild...|  0|[{document, 0, 80...|[{document, 0, 14...|[{token, 0, 2, 2-...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 10, 2...|[{named_entity, 0...|[{chunk, 0, 10, 2...|[{named_entity, 0...|[{chunk, 18, 23, ...|[{named_entity, 0...|[{chunk, 325, 333...|[{named_entity, 0...|            []|       []|[{document, 0, 10...|[{sentence_embedd...|[{entity, 0, 10, ...|[{entity, 0, 10, ...|  0.0|\n",
            "+---------------+-------------+--------------------+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oixb_1hr5BfL"
      },
      "source": [
        "#### Create Feature Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soZWLMGJvK4p"
      },
      "source": [
        "# Create features\n",
        "full_features = create_features(full_df, pipeline='full')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ufDzRIQEWbW"
      },
      "source": [
        "full_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNepbq9U-2_n"
      },
      "source": [
        "# Separate train/test\n",
        "full_train_features = full_features[full_features[\"source\"] == 'train']\n",
        "full_test_features = full_features[full_features[\"source\"] == 'test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcQ_qBbYvmnW"
      },
      "source": [
        "# Save features\n",
        "full_train_features.to_csv(\"/content/full_train_features.csv\", index=False)\n",
        "full_test_features.to_csv(\"/content/full_test_features.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRRIs-Cg-YS4"
      },
      "source": [
        "# Load features\n",
        "full_train_features = pd.read_csv(\"/content/full_train_features.csv\")\n",
        "full_test_features = pd.read_csv(\"/content/full_test_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEUk3ljt9IlB"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO0yaxXt9IlC"
      },
      "source": [
        "# Create train and test set\n",
        "X_train = full_train_features.drop(['label', 'class', 'entity_id', 'id', 'source'], axis=1)\n",
        "X_test = full_test_features.drop(['label', 'class', 'entity_id', 'id', 'source'], axis=1)\n",
        "y_train = full_train_features['class']\n",
        "y_test = full_test_features['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9BO2qOw9IlC"
      },
      "source": [
        "# Random forest classifier\n",
        "clf = skl_RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit to training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w78ZtX_S9IlC"
      },
      "source": [
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuaeM7JW9IlC"
      },
      "source": [
        "feature_imp = pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n",
        "feature_imp[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4XkF9Nk_TUZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT0qBKQo9IlC"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gZ_ERWO9IlC"
      },
      "source": [
        "# Logistic regression\n",
        "lr_clf = skl_LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Fit to training set\n",
        "lr_model = lr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_lr = lr_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38SPkicK9IlC"
      },
      "source": [
        "print(classification_report(y_test, y_pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4YZJx5B2grx"
      },
      "source": [
        "### No-resolver Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee_LTJUs8QHn"
      },
      "source": [
        "#### Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Cav97x8OGG"
      },
      "source": [
        "noresolver_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentenceDetector,\n",
        "        token,\n",
        "        embeddings,\n",
        "        clinical_ner,\n",
        "        clinical_converter,\n",
        "        jsl_ner,\n",
        "        jsl_converter,\n",
        "        bio_ner,\n",
        "        bio_converter,\n",
        "        posology_ner,\n",
        "        posology_converter,\n",
        "        risk_ner,\n",
        "        risk_converter,\n",
        "        risk_assertion,\n",
        "        label_stringIdx\n",
        "    ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcrzf5B8OGO"
      },
      "source": [
        "# Save unfit pipeline to disk\n",
        "noresolver_pipeline.save(\"/content/noresolver_pipeline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lJk53Nw-9OoZ",
        "outputId": "5cf158f9-c026-4486-a10f-d4b6460bdf17"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('/content/noresolver_pipeline', 'zip', '/content/noresolver_pipeline')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/noresolver_pipeline.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uNEC0jV8OGO"
      },
      "source": [
        "# Load unfit pipeline\n",
        "noresolver_pipeline = PipelineModel.load(\"/content/noresolver_pipeline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwp5UWpt8OGO"
      },
      "source": [
        "#### Fit Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiDpQPTK8OGO"
      },
      "source": [
        "# Limit dataset, add train/test differentiator\n",
        "limit_train = trainDataset.limit(500).withColumn(\"source\", lit(\"train\"))\n",
        "limit_test = testDataset.limit(500).withColumn(\"source\", lit(\"test\"))\n",
        "\n",
        "# Concat dataframes\n",
        "noresolver_unionDF = limit_train.union(limit_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_wgponr8OGO"
      },
      "source": [
        "# Fit pipeline\n",
        "noresolver_feature_model = noresolver_pipeline.fit(noresolver_unionDF)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE2eWD_W8OGO"
      },
      "source": [
        "# Transform train and test set\n",
        "noresolver_df = noresolver_feature_model.transform(noresolver_unionDF)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj6-3Osd8OGO",
        "outputId": "ef44055f-f467-4d84-ecaa-70b594089236"
      },
      "source": [
        "noresolver_df.show(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+--------------------+---+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+-----+\n",
            "|          label|  description|                text| id|source|            document|            sentence|         tok_checked|          embeddings|        clinical_ner|  clinical_ner_chunk|             jsl_ner|       jsl_ner_chunk|             bio_ner|       bio_ner_chunk|        posology_ner|  posology_ner_chunk|            risk_ner|risk_ner_chunk|assertion|class|\n",
            "+---------------+-------------+--------------------+---+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+-----+\n",
            "|Medical Records| 2-D Doppler |2-D STUDY,1. Mild...|  0| train|[{document, 0, 80...|[{document, 0, 14...|[{token, 0, 2, 2-...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 10, 2...|[{named_entity, 0...|[{chunk, 0, 10, 2...|[{named_entity, 0...|[{chunk, 18, 23, ...|[{named_entity, 0...|[{chunk, 325, 333...|[{named_entity, 0...|            []|       []|  1.0|\n",
            "+---------------+-------------+--------------------+---+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------+---------+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzbwtw6_8OGP"
      },
      "source": [
        "#### Create Feature Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8VYGjnupgSN"
      },
      "source": [
        "# Create features\n",
        "noresolver_features = create_features(noresolver_df, pipeline='no_resolver')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "zuOE-Jt28OGP",
        "outputId": "aededcfd-0e92-43b2-cc66-46d7bdb87333"
      },
      "source": [
        "noresolver_features.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>class</th>\n",
              "      <th>source</th>\n",
              "      <th>entity_PROBLEM</th>\n",
              "      <th>entity_TREATMENT</th>\n",
              "      <th>entity_TEST</th>\n",
              "      <th>entity_Multi-tissue_structure</th>\n",
              "      <th>entity_Organism</th>\n",
              "      <th>entity_Organ</th>\n",
              "      <th>entity_Simple_chemical</th>\n",
              "      <th>entity_Organism_subdivision</th>\n",
              "      <th>entity_Gene_or_gene_product</th>\n",
              "      <th>entity_Organism_substance</th>\n",
              "      <th>entity_DRUG</th>\n",
              "      <th>entity_Modifier</th>\n",
              "      <th>entity_Disease_Syndrome_Disorder</th>\n",
              "      <th>entity_Internal_organ_or_component</th>\n",
              "      <th>entity_Direction</th>\n",
              "      <th>entity_Symptom</th>\n",
              "      <th>entity_Heart_Disease</th>\n",
              "      <th>entity_Test</th>\n",
              "      <th>entity_Injury_or_Poisoning</th>\n",
              "      <th>entity_Clinical_Dept</th>\n",
              "      <th>entity_Test_Result</th>\n",
              "      <th>entity_Section_Header</th>\n",
              "      <th>entity_Gender</th>\n",
              "      <th>entity_Procedure</th>\n",
              "      <th>entity_Admission_Discharge</th>\n",
              "      <th>entity_Date</th>\n",
              "      <th>entity_External_body_part_or_region</th>\n",
              "      <th>entity_EKG_Findings</th>\n",
              "      <th>entity_Diet</th>\n",
              "      <th>entity_Medical_Device</th>\n",
              "      <th>entity_RelativeDate</th>\n",
              "      <th>entity_Drug_Ingredient</th>\n",
              "      <th>entity_PHI</th>\n",
              "      <th>entity_MEDICATION</th>\n",
              "      <th>entity_present</th>\n",
              "      <th>entity_absent</th>\n",
              "      <th>...</th>\n",
              "      <th>entity_VS_Finding</th>\n",
              "      <th>entity_Treatment</th>\n",
              "      <th>entity_Oxygen_Therapy</th>\n",
              "      <th>entity_O2_Saturation</th>\n",
              "      <th>entity_Time</th>\n",
              "      <th>entity_Psychological_Condition</th>\n",
              "      <th>entity_Substance</th>\n",
              "      <th>entity_CAD</th>\n",
              "      <th>entity_Developing_anatomical_structure</th>\n",
              "      <th>entity_Allergen</th>\n",
              "      <th>entity_Vaccine</th>\n",
              "      <th>entity_Diabetes</th>\n",
              "      <th>entity_Obesity</th>\n",
              "      <th>entity_Relationship_Status</th>\n",
              "      <th>entity_OBESE</th>\n",
              "      <th>entity_SMOKER</th>\n",
              "      <th>entity_DIABETES</th>\n",
              "      <th>entity_Temperature</th>\n",
              "      <th>entity_Cerebrovascular_Disease</th>\n",
              "      <th>entity_ImagingFindings</th>\n",
              "      <th>entity_Hypertension</th>\n",
              "      <th>entity_Communicable_Disease</th>\n",
              "      <th>entity_Social_History_Header</th>\n",
              "      <th>entity_Alcohol</th>\n",
              "      <th>entity_HYPERTENSION</th>\n",
              "      <th>entity_Imaging_Technique</th>\n",
              "      <th>entity_Fetus_NewBorn</th>\n",
              "      <th>entity_Kidney_Disease</th>\n",
              "      <th>entity_Death_Entity</th>\n",
              "      <th>entity_Hyperlipidemia</th>\n",
              "      <th>entity_Total_Cholesterol</th>\n",
              "      <th>entity_Height</th>\n",
              "      <th>entity_LDL</th>\n",
              "      <th>entity_Triglycerides</th>\n",
              "      <th>entity_HDL</th>\n",
              "      <th>entity_HYPERLIPIDEMIA</th>\n",
              "      <th>entity_Birth_Entity</th>\n",
              "      <th>entity_Sexually_Active_or_Sexual_Orientation</th>\n",
              "      <th>entity_BMI</th>\n",
              "      <th>entity_Overweight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Medical Records</td>\n",
              "      <td>1.0</td>\n",
              "      <td>train</td>\n",
              "      <td>32.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Medical Records</td>\n",
              "      <td>1.0</td>\n",
              "      <td>test</td>\n",
              "      <td>32.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Surgery</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "      <td>41.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Surgery</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>41.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Medical Records</td>\n",
              "      <td>1.0</td>\n",
              "      <td>train</td>\n",
              "      <td>48.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 117 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id            label  ...  entity_BMI entity_Overweight\n",
              "0   0  Medical Records  ...         0.0               0.0\n",
              "1   0  Medical Records  ...         0.0               0.0\n",
              "2   1          Surgery  ...         0.0               0.0\n",
              "3   1          Surgery  ...         0.0               0.0\n",
              "4   2  Medical Records  ...         0.0               0.0\n",
              "\n",
              "[5 rows x 117 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiX0uUsl8OGP"
      },
      "source": [
        "# Separate train/test\n",
        "noresolver_train_features = noresolver_features[noresolver_features[\"source\"] == 'train']\n",
        "noresolver_test_features = noresolver_features[noresolver_features[\"source\"] == 'test']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg05ltbxY1C3"
      },
      "source": [
        "# Save features\n",
        "noresolver_train_features.to_csv(\"/content/noresolver_train_features.csv\", index=False)\n",
        "noresolver_test_features.to_csv(\"/content/noresolver_test_features.csv\", index=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRmYC1BoYVZ"
      },
      "source": [
        "# Load features\n",
        "noresolver_train_features = pd.read_csv(\"/content/noresolver_train_features.csv\")\n",
        "noresolver_test_features = pd.read_csv(\"/content/noresolver_test_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnswv5PVtqPW"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHer1iZ2shjD"
      },
      "source": [
        "# Create train and test set\n",
        "X_train = noresolver_train_features.drop(['label', 'class', 'entity_id', 'id', 'source'], axis=1)\n",
        "X_test = noresolver_test_features.drop(['label', 'class', 'entity_id', 'id', 'source'], axis=1)\n",
        "y_train = noresolver_train_features['class']\n",
        "y_test = noresolver_test_features['class']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoWe4QasyO7R"
      },
      "source": [
        "# Random forest classifier\n",
        "clf = skl_RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit to training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = clf.predict(X_test)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kKfjjE6yWSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4b8e64-2ece-4069-fb0c-f189df8e048a"
      },
      "source": [
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.42      0.33      0.37       198\n",
            "         1.0       0.24      0.31      0.27       102\n",
            "         2.0       0.19      0.18      0.18       109\n",
            "         3.0       0.19      0.21      0.20        91\n",
            "\n",
            "    accuracy                           0.27       500\n",
            "   macro avg       0.26      0.26      0.26       500\n",
            "weighted avg       0.29      0.27      0.28       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n5LB6J4wjHQ",
        "outputId": "89242739-beb7-42f8-b98c-b1244bfe0a44"
      },
      "source": [
        "feature_imp = pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n",
        "feature_imp[:20]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "entity_TREATMENT                       0.032925\n",
              "entity_Medical_Device                  0.032232\n",
              "entity_Organ                           0.026649\n",
              "entity_Direction                       0.024935\n",
              "entity_Internal_organ_or_component     0.024422\n",
              "entity_Gender                          0.022633\n",
              "entity_Procedure                       0.022547\n",
              "entity_TEST                            0.022206\n",
              "entity_Simple_chemical                 0.021396\n",
              "entity_Symptom                         0.021187\n",
              "entity_Multi-tissue_structure          0.021168\n",
              "entity_PROBLEM                         0.020336\n",
              "entity_External_body_part_or_region    0.020122\n",
              "entity_Modifier                        0.019545\n",
              "entity_Tissue                          0.018786\n",
              "entity_Disease_Syndrome_Disorder       0.018508\n",
              "entity_Test                            0.018285\n",
              "entity_Organism                        0.018002\n",
              "entity_DRUG                            0.017398\n",
              "entity_Injury_or_Poisoning             0.017363\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiehBId_tuyW"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHU3LM7BtkCO"
      },
      "source": [
        "# Logistic regression\n",
        "lr_clf = skl_LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Fit to training set\n",
        "lr_model = lr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_lr = lr_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMDcl0G2wzcu",
        "outputId": "b52d81ca-51ec-4106-a195-41934fdd8612"
      },
      "source": [
        "print(classification_report(y_test, y_pred_lr))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.47      0.49       198\n",
            "         1.0       0.29      0.37      0.33       102\n",
            "         2.0       0.21      0.18      0.19       109\n",
            "         3.0       0.25      0.24      0.24        91\n",
            "\n",
            "    accuracy                           0.35       500\n",
            "   macro avg       0.31      0.32      0.31       500\n",
            "weighted avg       0.35      0.35      0.35       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-TFsFwsmS5x"
      },
      "source": [
        "## Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMX7HSA9f0Qa"
      },
      "source": [
        "To compare the performance of the different models and account for the class imbalance, we can focus on the weighted average of the F1 score. The top 3 performing models were:\n",
        "\n",
        "1. Logistic Regression with Universal Sentence Encoder (0.50)\n",
        "2. DL Classification with BERT Sentence Embeddings (0.45)\n",
        "3. DL Classification with BioBERT Clnical Sentence Embeddings (0.45)\n",
        "\n",
        "All models performed poorly overall possibly due to the small sample size and slight class imbalances. \n",
        "\n",
        "The final step in the analysis was to create features using 5 different clinical NER models, a clinical risk assertion model and 2 clinical entity resolvers. This step was memory intensive and required excessive amounts of runtime which made it difficult to run on the full train and test sets. The results presented are therefore for limited (500 row) train and test sets."
      ]
    }
  ]
}